{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datasets transformers[tf,torch,sentencepiece,vision,optuna,sklearn,onnxruntime]==4.11.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from utils import *\n",
    "setup_chapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this book we will demonstrate how you can run the example from the book in Amazon SageMaker. \n",
    "\n",
    "The SageMaker notebook uses an AWS IAM role to access AWS resources such as Amazon S3 bucket.\n",
    "You created this role during the notebook creation process described in the README.md in SageMaker/README.md.\n",
    "In the AWS IAM service you are able to review the access policy and you can modify it.\n",
    "\n",
    "In the next cell we will check an Amazon S3 bucket exists and create a new one if not. In addition we'll get the SageMaker role and session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.151.0.tar.gz (747 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m748.0/748.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting attrs<23,>=20.3.0 (from sagemaker)\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3<2.0,>=1.26.28 (from sagemaker)\n",
      "  Using cached boto3-1.26.124-py3-none-any.whl (135 kB)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting google-pasta (from sagemaker)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from sagemaker) (1.24.3)\n",
      "Collecting protobuf<4.0,>=3.1 (from sagemaker)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting protobuf3-to-dict<1.0,>=0.1.5 (from sagemaker)\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting smdebug_rulesconfig==1.0.1 (from sagemaker)\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata<5.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from sagemaker) (23.1)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (from sagemaker) (2.0.1)\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting schema (from sagemaker)\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting PyYAML==5.4.1 (from sagemaker)\n",
      "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /home/codespace/.local/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /home/codespace/.local/lib/python3.10/site-packages (from sagemaker) (3.3.0)\n",
      "Collecting tblib==1.7.0 (from sagemaker)\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.124 (from boto3<2.0,>=1.26.28->sagemaker)\n",
      "  Using cached botocore-1.29.124-py3-none-any.whl (10.7 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.26.28->sagemaker)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3<2.0,>=1.26.28->sagemaker)\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<5.0,>=1.4.0->sagemaker)\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Collecting ppft>=1.7.6.6 (from pathos->sagemaker)\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill>=0.3.6 (from pathos->sagemaker)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pox>=0.3.2 (from pathos->sagemaker)\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "Collecting multiprocess>=0.70.14 (from pathos->sagemaker)\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting contextlib2>=0.5.5 (from schema->sagemaker)\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/codespace/.local/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.124->boto3<2.0,>=1.26.28->sagemaker) (1.26.15)\n",
      "Building wheels for collected packages: sagemaker, PyYAML, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.151.0-py2.py3-none-any.whl size=1003758 sha256=5bfef354ad0db3c0c5de523698d17b601ef9495270eb7c2235c8629a46b6118e\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/77/e9/bb/01713c249d9194892f8a12bc7a02e13d3278a878638ed1fa13\n",
      "  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyYAML: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=568690 sha256=dcfcbf9955e41db75eb1a2726165e75a2c281c53ded902cd8994ae58182cfb12\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
      "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4016 sha256=8eab53a649d3e1335e4ca2e4057f36763251fcef6276fae9e392b57b0ba254b8\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/12/3e/42/e783cdd4e7b8fda9bfc472eeb465bc9041bda90a3dbece8d74\n",
      "Successfully built sagemaker PyYAML protobuf3-to-dict\n",
      "Installing collected packages: zipp, tblib, smdebug_rulesconfig, PyYAML, protobuf, ppft, pox, jmespath, google-pasta, dill, contextlib2, cloudpickle, attrs, schema, protobuf3-to-dict, multiprocess, importlib-metadata, botocore, s3transfer, pathos, boto3, sagemaker\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed PyYAML-5.4.1 attrs-22.2.0 boto3-1.26.124 botocore-1.29.124 cloudpickle-2.2.1 contextlib2-21.6.0 dill-0.3.6 google-pasta-0.2.0 importlib-metadata-4.13.0 jmespath-1.0.1 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 protobuf-3.20.3 protobuf3-to-dict-0.1.5 s3transfer-0.6.0 sagemaker-2.151.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 tblib-1.7.0 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::123456789012:role/my-sagemaker-role\n",
      "sagemaker bucket: my-sagemaker-bucket\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker.huggingface\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup a helper function to easily deploy any Hugging Face model as an endpoint on AWS SageMaker.\n",
    "We use the following function to create a HuggingFaceModel Class, where we are going to download the model from the Hugging Face hub. This class would also allow to use a trained model stored on the Amazon S3 bucket. \n",
    "Next, an endpoint will be created and this endpoint will host your model. Based on the model requirements you can choose a specific instance type which are equipped differently in memory, cpu, gpu. There are different inferences available, such as real-time, asynchronous or serverless.\n",
    "If you are not sure which inference works best for your model, you use Amazon SageMaker Inference Recommender.\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html \n",
    "\n",
    "To view all options see the documentation: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html \n",
    "\n",
    "Depending on Transformer version, PyTorch/TensorFlow version and Python version, the mapping for the Hugging Face Model Class can be found here: https://huggingface.co/docs/sagemaker/reference#inference-dlc-overview \n",
    "\n",
    "To find the endpoints in the AWS Console navigate to https://console.aws.amazon.com/sagemaker/home#/endpoints \n",
    "\n",
    "Make sure to finish this notebook to delete the endpoint in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "def setup_endpoint(model_name, task_name):\n",
    "    # Hub Model configuration. <https://huggingface.co/models>\n",
    "    hub = {\n",
    "      'HF_MODEL_ID': model_name, # model_id from hf.co/models\n",
    "      'HF_TASK': task_name # NLP task you want to use for predictions\n",
    "    }\n",
    "\n",
    "    # create Hugging Face Model Class\n",
    "    huggingface_model = HuggingFaceModel(\n",
    "       env=hub, # configuration for loading model from Hub\n",
    "       role=role, # iam role with permissions to create an Endpoint\n",
    "       transformers_version=\"4.17.0\", # transformers version used\n",
    "       pytorch_version=\"1.10.2\", # pytorch version used\n",
    "       py_version=\"py38\" # python version used\n",
    "    )\n",
    "\n",
    "    # deploy model to SageMaker Inference\n",
    "    predictor = huggingface_model.deploy(\n",
    "       initial_instance_count=1, # how many instances used\n",
    "       instance_type=\"ml.m5.xlarge\" # instance type\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder-Decoder Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers: Bridging the Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tour of Transformer Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = setup_endpoint('distilbert-base-uncased-finetuned-sst-2-english', 'text-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "import pandas as pd\n",
    "\n",
    "# request\n",
    "outputs = predictor.predict({\"inputs\": text})\n",
    "pd.DataFrame(outputs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = setup_endpoint(\"dbmdz/bert-large-cased-finetuned-conll03-english\", \"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor.predict({\"inputs\": text, \"parameters\": {\"aggregation_strategy\": \"simple\"}})\n",
    "pd.DataFrame(outputs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = setup_endpoint(\"distilbert-base-cased-distilled-squad\", 'question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does the customer want?\"\n",
    "\n",
    "outputs = predictor.predict({\"inputs\": {\n",
    "    \"question\": question,\n",
    "    \"context\": text\n",
    "    }\n",
    "})\n",
    "\n",
    "pd.DataFrame([outputs])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = setup_endpoint(\"sshleifer/distilbart-cnn-12-6\", 'summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor.predict({\"inputs\": text,\n",
    "                             \"parameters\": {\n",
    "                                 \"max_length\":45,\n",
    "                                 \"clean_up_tokenization_spaces\":True\n",
    "                                 }\n",
    "                            })\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = setup_endpoint(\"Helsinki-NLP/opus-mt-en-de\", \"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor.predict({\"inputs\": text,\n",
    "                             \"parameters\": {\n",
    "                                 \"min_length\":100,\n",
    "                                 \"clean_up_tokenization_spaces\":True\n",
    "                                 }\n",
    "                            })\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = setup_endpoint(\"gpt2\", 'text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "\n",
    "outputs = predictor.predict({\"inputs\": prompt,\n",
    "                             \"parameters\": {\n",
    "                                 \"max_length\":200\n",
    "                                 }\n",
    "                            })\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hugging Face Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Challenges with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
